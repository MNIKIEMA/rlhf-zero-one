{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63ceecbc-87ad-4ad3-a317-f49267ffc93b",
   "metadata": {},
   "source": [
    "# Agent Training with GRPO using TRL\n",
    "\n",
    "![trl banner](https://huggingface.co/datasets/trl-lib/documentation-images/resolve/main/trl_banner_dark.png)\n",
    "\n",
    "\n",
    "With [**Transformers Reinforcement Learning (TRL)**](https://github.com/huggingface/trl), you can train a language model to act as an **agent**. One that learns to reason, interact with external tools, and improve through reinforcement.\n",
    "\n",
    "- [TRL GitHub Repository](https://github.com/huggingface/trl) â€” star us to support the project!  \n",
    "- [Official TRL Examples](https://huggingface.co/docs/trl/example_overview)  \n",
    "- [Community Tutorials](https://huggingface.co/docs/trl/community_tutorials)\n",
    "- [OpenEnv](https://github.com/meta-pytorch/OpenEnv)\n",
    "\n",
    "\n",
    "TRL supports training agents that can use external tools as part of their decision process.  \n",
    "In this notebook, the agent has access to the **BioGRID database**, which it can query using **read-only SQL commands** to retrieve biological interaction data. The model learns when and how to use tools based on rewards.\n",
    "\n",
    "We'll fine-tune a model using GRPO (Group Relative Policy Optimization) via TRL. The agent will:\n",
    "\n",
    "1. Generate tool call to query the database if needed.\n",
    "2. Receive the tool response and add it it to the context.\n",
    "3. Learn to improve its tool usage and general capabilities over time through reward signals.\n",
    "\n",
    "## Install dependencies\n",
    "\n",
    "We'll start by installing **TRL**, which automatically includes the main dependencies like **Transformers**.  \n",
    "We'll also install **trackio** (for logging and monitoring training runs), **vLLM** (for efficient generation), and **jmespath** (needed for the tools capabilities)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4812fbf-3f61-481e-9a64-95277eada9c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!uv pip install -Uq \"trl[vllm]\" \"transformers>=5.2.0\" trackio jmespath \"numpy==1.26.4\" duckdb \"smolagents>=1.24.0\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede8e566-a1b5-460f-9fe8-a6010bc56148",
   "metadata": {},
   "source": [
    "### Log in to Hugging Face\n",
    "\n",
    "Log in to your **Hugging Face** account to save your fine-tuned model, track your experiment results directly on the Hub or access gated models. You can find your **access token** on your [account settings page](https://huggingface.co/settings/tokens)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21756ac0-78b2-495d-8137-28dfa9faae6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "KVGklspLYlmz",
   "metadata": {},
   "source": [
    "## Create the database for the tool\n",
    "\n",
    "For this example, we will use the [BioGRID database](https://thebiogrid.org/), a curated resource containing **protein, genetic, and chemical interaction data**.  We've already compiled and uploaded it to the Hub at [qgallouedec/biogrid](https://huggingface.co/datasets/qgallouedec/biogrid). The dataset is loaded and converted into an sqlite database.\n",
    "\n",
    "> ðŸ’¡ We remove spaces in the column names to easen the model work. In real-world deployments, you may keep your original column names and rely on the agent to reason about them. Here, we simplify the schema to make training smoother."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "rRzPMhfXBLkF",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1a70f1c28e34bc08462f09a821095e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "biogrid.duckdb created. Rows stored: 2815790\n"
     ]
    }
   ],
   "source": [
    "import duckdb\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load dataset\n",
    "biogrid_dataset = load_dataset(\"qgallouedec/biogrid\", split=\"train\")\n",
    "df = biogrid_dataset.to_pandas()\n",
    "\n",
    "# Normalize column names: remove spaces, replace with underscores\n",
    "df.columns = [c.replace(\" \", \"_\") for c in df.columns]\n",
    "\n",
    "# Save to SQLite\n",
    "conn = duckdb.connect(\"biogrid.duckdb\")\n",
    "try:\n",
    "    conn.execute(\"CREATE OR REPLACE TABLE interactions AS SELECT * FROM df\")\n",
    "    print(f\"biogrid.duckdb created. Rows stored: {len(df)}\")\n",
    "finally:\n",
    "    conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pSSGvLbmZyC2",
   "metadata": {},
   "source": [
    "## Load the QA dataset\n",
    "\n",
    "The training objective is to fine-tune a model to answer gene-related questions. The model should learn to use the database query tool to retrieve factual information when needed.\n",
    "\n",
    "We'll define a formatting function for each sample, adding instructions about the database and how to call it. The model must answer with **yes** or **no**. Let's implement the `format_example` function.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "asrv7LbaD71C",
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap\n",
    "\n",
    "def format_example(example):\n",
    "    question = example[\"question\"]\n",
    "    preamble = textwrap.dedent(\"\"\"\\\n",
    "    You have access to the BioGRID SQLite database.\n",
    "    Use SQL queries to retrieve only the information needed to answer the question.\n",
    "\n",
    "    Genes may appear in the database in columns `Alt_IDs_Interactor_A` `Alt_IDs_Interactor_B`, `Aliases_Interactor_A` and `Aliases_Interactor_B`,\n",
    "    and each entry can contain multiple gene names or synonyms separated by '|', for example:\n",
    "    'entrez gene/locuslink:JNKK(gene name synonym)|entrez gene/locuslink:MAPKK4(gene name synonym)|...'\n",
    "    So a gene like 'JNKK' or 'MAPKK4' may appear inside one of these strings.\n",
    "\n",
    "    If the database schema is unclear or you are unsure about column names:\n",
    "    - First inspect the schema with `PRAGMA table_info(interactions);`\n",
    "    - Or preview a few rows with `SELECT * FROM interactions LIMIT 1;`\n",
    "\n",
    "    Otherwise, directly query the required data.\n",
    "\n",
    "    Final answer must be enclosed in stars, e.g. *Yes* or *No*.\n",
    "    Facts:\n",
    "    - The NCBI Taxonomy identifier for humans is taxid:9606.\n",
    "    \"\"\")\n",
    "    content = f\"{preamble}\\nQuestion: {question}\"\n",
    "    prompt = [{\"role\": \"user\", \"content\": content}]\n",
    "    return {\"prompt\": prompt}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "UMnHXYZla_EO",
   "metadata": {},
   "source": [
    "Now, let's load the database and call the previous function.  \n",
    "For simplicity, we will only use questions that start with **â€œDoes the geneâ€¦â€**.  \n",
    "In a real use case, the full dataset can be used.\n",
    "\n",
    "The QA dataset is available on the [Hub](https://huggingface.co/datasets/qgallouedec/biogrid_qa)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "jEs12KqwDnVl",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"qgallouedec/biogrid_qa\", split=\"train\")\n",
    "dataset = dataset.filter(\n",
    "    lambda example: example[\"question\"].startswith(\"Does the gene \")\n",
    ")  # keep only simple questions for example\n",
    "dataset = dataset.map(format_example, remove_columns=[\"question\"])\n",
    "\n",
    "train_dataset = dataset\n",
    "eval_dataset = None  # No eval by default, can be added if needed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "m4GRjbHycM5L",
   "metadata": {},
   "source": [
    "## Create tool for the agent\n",
    "\n",
    "The `query_biogrid` function is the tool the model will use to query the database and retrieve factual information.  \n",
    "Each tool must be a standard Python function with **type-hinted arguments and return types**, and a **Google-style docstring** describing its purpose, parameters, and return value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "nLMH7hahGTyO",
   "metadata": {},
   "outputs": [],
   "source": [
    "from contextlib import contextmanager\n",
    "import signal\n",
    "\n",
    "@contextmanager\n",
    "def timeout(seconds):\n",
    "    \"\"\"Context manager that raises TimeoutError if execution exceeds time limit.\"\"\"\n",
    "\n",
    "    def timeout_handler(signum, frame):\n",
    "        raise TimeoutError(f\"Operation timed out after {seconds} seconds\")\n",
    "\n",
    "    signal.signal(signal.SIGALRM, timeout_handler)\n",
    "    signal.alarm(seconds)\n",
    "    try:\n",
    "        yield\n",
    "    finally:\n",
    "        signal.alarm(0)\n",
    "\n",
    "def query_biogrid(sql_command: str) -> list[tuple]:\n",
    "    \"\"\"\n",
    "    Execute a read-only SQL command on the BioGRID database.\n",
    "\n",
    "    BioGRID is a curated biological database that compiles protein, genetic, and chemical interactions from multiple organisms. It provides researchers with experimentally verified interaction data to support studies in systems biology and functional genomics.\n",
    "\n",
    "    Args:\n",
    "        sql_command: The SQL command to execute.\n",
    "\n",
    "    Returns:\n",
    "        A list of tuples containing the query results.\n",
    "    \"\"\"\n",
    "    with timeout(5):\n",
    "        conn = duckdb.connect(\"biogrid.duckdb\", read_only=True)\n",
    "        try:\n",
    "            results = conn.execute(sql_command).fetchall()\n",
    "        finally:\n",
    "            conn.close()\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "GiHtooTwci3B",
   "metadata": {},
   "source": [
    "## Define reward functions\n",
    "\n",
    "To guide the agent during training, we define a few simple reward functions:\n",
    "\n",
    "- **`query_reward`**: evaluates the modelâ€™s query strategy â€” penalizes more than two queries, penalizes generic database scans, and rewards use of `WHERE` and evidence supporting the final answer.\n",
    "- **`correctness_reward`**: rewards Yes/No predictions that match the expected answer.\n",
    "- **`structure_reward`**: rewards a proper assistant structure (tool call â†’ response â†’ optional explanation).\n",
    "\n",
    "Each function returns a list of floats used by the **GRPOTrainer** during optimization.  \n",
    "Combined, they encourage effective tool use and factual answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "sXyqC6cJGe3L",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def query_reward(completions, answer, **kwargs):\n",
    "    \"\"\"\n",
    "    Reward query strategy:\n",
    "    - Penalize more than 2 queries\n",
    "    - Penalize generic queries (LIMIT 1 / PRAGMA)\n",
    "    - Reward usage of WHERE\n",
    "    - Reward evidence supporting the final answer\n",
    "    \"\"\"\n",
    "    rewards = []\n",
    "\n",
    "    for completion, ans in zip(completions, answer, strict=False):\n",
    "        reward = 0.0\n",
    "        sql_queries = []\n",
    "        tool_results = []\n",
    "\n",
    "        # collect all SQL queries and tool results\n",
    "        for turn in completion:\n",
    "            if turn.get(\"tool_calls\"):\n",
    "                for call in turn[\"tool_calls\"]:\n",
    "                    sql = call[\"function\"][\"arguments\"].get(\"sql_command\", \"\").lower()\n",
    "                    sql_queries.append(sql)\n",
    "            if turn.get(\"role\") == \"tool\" and turn.get(\"content\"):\n",
    "                tool_results.append(turn[\"content\"])\n",
    "\n",
    "        # --- penalize too many queries ---\n",
    "        if len(sql_queries) > 3:\n",
    "            reward -= 1.5\n",
    "\n",
    "        # --- check query quality ---\n",
    "        where_count = 0\n",
    "        for q in sql_queries:\n",
    "            if \"limit 1\" in q:\n",
    "                reward -= 1.0\n",
    "            if \" where \" not in q:\n",
    "                reward -= 0.5\n",
    "            else:\n",
    "                where_count += 1\n",
    "        reward += min(where_count, 3) * 0.4  # small bonus for WHERE usage\n",
    "\n",
    "        # --- evidence check: do queries support the answer? ---\n",
    "        combined_results = []\n",
    "        error_detected = False\n",
    "\n",
    "        for res in tool_results:\n",
    "            if isinstance(res, dict) and \"error\" in res:\n",
    "                error_detected = True\n",
    "            elif isinstance(res, list):\n",
    "                combined_results.extend(res)\n",
    "\n",
    "        # if error detected, penalize heavily\n",
    "        if error_detected:\n",
    "            reward -= 2.0\n",
    "        elif len(sql_queries) == 0:\n",
    "            reward -= 1.5\n",
    "        else:\n",
    "            has_hits = len(combined_results) > 0\n",
    "            correct_answer = ans.lower()\n",
    "            if (has_hits and correct_answer == \"yes\") or (not has_hits and correct_answer == \"no\"):\n",
    "                reward += 2.0\n",
    "            else:\n",
    "                reward -= 1.5\n",
    "\n",
    "        rewards.append(reward)\n",
    "\n",
    "    return rewards\n",
    "\n",
    "\n",
    "def correctness_reward(completions, answer, **kwargs):\n",
    "    \"\"\"\n",
    "    Reward Yes/No correctness.\n",
    "    Model must provide final answer enclosed in stars â€” *yes* or *no*.\n",
    "    Does not reward informal yes/no buried in text.\n",
    "    \"\"\"\n",
    "    rewards = []\n",
    "    for completion, ans in zip(completions, answer, strict=False):\n",
    "        raw = completion[-1][\"content\"].lower()\n",
    "\n",
    "        # detect form *yes* or *no*\n",
    "        match = re.search(r\"\\*(yes|no)\\*\", raw)\n",
    "        guess = match.group(1) if match else None\n",
    "\n",
    "        reward = 0.0\n",
    "\n",
    "        if guess is None:\n",
    "            reward -= 0.5  # invalid format\n",
    "        elif guess == ans.lower():\n",
    "            reward += 0.6  # correct under required format\n",
    "        else:\n",
    "            reward -= 1.0  # wrong answer\n",
    "\n",
    "        rewards.append(reward)\n",
    "\n",
    "    return rewards\n",
    "\n",
    "\n",
    "def structure_reward(completions, **kwargs):\n",
    "    \"\"\"\n",
    "    Reward proper assistant structure.\n",
    "    Encourages a logical sequence: tool call + response + optional extra content.\n",
    "    \"\"\"\n",
    "    rewards = []\n",
    "\n",
    "    for completion in completions:\n",
    "        has_call = False\n",
    "        has_response = False\n",
    "        has_other = False\n",
    "\n",
    "        for turn in completion:\n",
    "            role = turn.get(\"role\")\n",
    "            if role == \"assistant\" and turn.get(\"tool_calls\"):\n",
    "                has_call = True\n",
    "            elif role == \"tool\":\n",
    "                has_response = True\n",
    "            else:\n",
    "                content = turn.get(\"content\")\n",
    "                if content and content.strip() not in [\"\", \"<think>\"]:\n",
    "                    has_other = True\n",
    "\n",
    "        # Reward sequences\n",
    "        if has_call and has_response:\n",
    "            if has_other:\n",
    "                reward = 0.1\n",
    "            else:\n",
    "                reward = 0.05  # still positive even without extra text\n",
    "        elif has_call and not has_response:\n",
    "            reward = -0.15\n",
    "        else:\n",
    "            reward = 0.0  # neutral if no call\n",
    "\n",
    "        rewards.append(reward)\n",
    "\n",
    "    return rewards\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "zcgkrKtTb4T9",
   "metadata": {},
   "source": [
    "## Set GRPO Config\n",
    "\n",
    "Next, we define the **GRPOConfig**, which controls the main training parameters.  \n",
    "This configuration specifies how the model interacts with **vLLM**, manages memory, and logs results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "t4ifJsNLElIN",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import GRPOConfig\n",
    "\n",
    "output_dir = \"grpo_biogrid_qwen_3g-1.7b\"\n",
    "\n",
    "grpo_config = GRPOConfig(\n",
    "    # Training schedule / optimization\n",
    "    max_steps=200,                                              # Max number of training steps\n",
    "    chat_template_kwargs = {\"enable_thinking\": False},          # Disable thinking to reduce token generation\n",
    "\n",
    "    # GRPO configuration\n",
    "    max_completion_length = 1024,                               # Maximum tokens generated per model response\n",
    "\n",
    "    # vLLM configuration\n",
    "    use_vllm = True,                                            # Enable vLLM for faster inference during rollouts\n",
    "    vllm_mode = \"colocate\",                                     # Run vLLM in colocate mode (same process as training)\n",
    "    vllm_enable_sleep_mode=False,\n",
    "\n",
    "    # Logging / reporting\n",
    "    output_dir = output_dir,                                    # Directory for checkpoints and logs\n",
    "    report_to=\"trackio\",                                        # Experiment tracking tool (integrates with HF Spaces)\n",
    "    trackio_space_id = output_dir,                              # HF Space where experiment tracking will be saved\n",
    "    save_steps = 10,                                            # Interval for saving checkpoints\n",
    "    log_completions = True,\n",
    "\n",
    "    # Memory optimization\n",
    "    gradient_checkpointing = True,                              # Enable activation recomputation to save memory\n",
    "\n",
    "    # Hub integration\n",
    "    push_to_hub = True,                                         # Set True to automatically push model to Hugging Face Hub\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34I-Q2MJuf42",
   "metadata": {},
   "source": [
    "## Create `GRPOTrainer` and Start Training\n",
    "\n",
    "Next, we initialize the **`GRPOTrainer`**, which handles the full reinforcement learning loop.\n",
    "\n",
    "It receives the model name, reward functions, tool(s), and dataset defined earlier.  \n",
    "\n",
    "Finally, we call `trainer.train()` to begin fine-tuning, allowing the model to learn how to query the database effectively through iterative feedback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "IysntAUOFvRn",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ff2b6e90aa9472b9d07a7c1d8fd46ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (incomplete total...): 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd80de99b0fa47c6906ff5ebb5389864",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc711172ff414a56b1ed051cf5b22003",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/311 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to lm_head.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR 02-19 11:10:01 [fa_utils.py:57] Cannot use FA version 2 is not supported due to FA2 is only supported on devices with compute capability >= 8\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b66a38d3f2e44c8a8e1169c443dbaf07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank0]:W0219 11:10:43.661000 168481 /system/conda/miniconda3/uv/cache/archive-v0/1gOSl5RmB3s_zVV6bng4j/torch/_inductor/utils.py:1436] [0/0] Not enough SMs to use max_autotune_gemm mode\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "To serve at least one request with the models's max seq len (40960), (4.38 GiB KV cache is needed, which is larger than the available KV cache memory (0.96 GiB). Based on the available memory, the estimated maximum model length is 8976. Try increasing `gpu_memory_utilization` or decreasing `max_model_len` when initializing the engine.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtrl\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m GRPOTrainer\n\u001b[1;32m      3\u001b[0m model_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQwen/Qwen3-1.7B\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 5\u001b[0m trainer \u001b[38;5;241m=\u001b[39m \u001b[43mGRPOTrainer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtools\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mquery_biogrid\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreward_funcs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mcorrectness_reward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstructure_reward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery_reward\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrpo_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/trl/trainer/grpo_trainer.py:661\u001b[0m, in \u001b[0;36mGRPOTrainer.__init__\u001b[0;34m(self, model, reward_funcs, args, train_dataset, eval_dataset, processing_class, reward_processing_classes, callbacks, optimizers, peft_config, tools, rollout_func)\u001b[0m\n\u001b[1;32m    658\u001b[0m         \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mrollout_func\u001b[39m(prompts):\n\u001b[1;32m    659\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrollout_func(prompts, \u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m--> 661\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvllm_generation \u001b[38;5;241m=\u001b[39m \u001b[43mVLLMGeneration\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    662\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    663\u001b[0m \u001b[43m        \u001b[49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    664\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_fsdp_enabled\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_fsdp_enabled\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    665\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprocessing_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocessing_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    666\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# vLLM configuration\u001b[39;49;00m\n\u001b[1;32m    667\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvllm_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    668\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstructured_outputs_regex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvllm_structured_outputs_regex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    669\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Server mode configuration\u001b[39;49;00m\n\u001b[1;32m    670\u001b[0m \u001b[43m        \u001b[49m\u001b[43mserver_base_url\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvllm_server_base_url\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    671\u001b[0m \u001b[43m        \u001b[49m\u001b[43mserver_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvllm_server_host\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    672\u001b[0m \u001b[43m        \u001b[49m\u001b[43mserver_port\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvllm_server_port\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    673\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgroup_port\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvllm_group_port\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[43m        \u001b[49m\u001b[43mserver_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvllm_server_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    675\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Colocate mode configuration\u001b[39;49;00m\n\u001b[1;32m    676\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtensor_parallel_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvllm_tensor_parallel_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    677\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgpu_memory_utilization\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvllm_gpu_memory_utilization\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    678\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_model_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvllm_max_model_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    679\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_num_seqs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mper_device_train_batch_size\u001b[49m\n\u001b[1;32m    680\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvllm_tensor_parallel_size\u001b[49m\n\u001b[1;32m    681\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msteps_per_generation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    682\u001b[0m \u001b[43m        \u001b[49m\u001b[43menable_sleep_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvllm_enable_sleep_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    683\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_impl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvllm_model_impl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    684\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Generation configuration\u001b[39;49;00m\n\u001b[1;32m    685\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepetition_penalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrepetition_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    686\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    687\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    688\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtop_k\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    689\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmin_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmin_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    690\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_completion_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_completion_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    691\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgeneration_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    692\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Chat/tool configuration\u001b[39;49;00m\n\u001b[1;32m    693\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchat_template\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat_template\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    694\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchat_template_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat_template_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    695\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtools\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    696\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrollout_func\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrollout_func\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    697\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    698\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_last_loaded_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m  \u001b[38;5;66;03m# tag to avoid useless loading during grad accumulation\u001b[39;00m\n\u001b[1;32m    699\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/trl/generation/vllm_generation.py:257\u001b[0m, in \u001b[0;36mVLLMGeneration.__init__\u001b[0;34m(self, model, accelerator, is_fsdp_enabled, processing_class, mode, structured_outputs_regex, server_base_url, server_host, server_port, server_timeout, group_port, tensor_parallel_size, gpu_memory_utilization, max_model_length, max_num_seqs, enable_sleep_mode, model_impl, repetition_penalty, temperature, top_p, top_k, min_p, max_completion_length, generation_kwargs, chat_template, chat_template_kwargs, tools, rollout_func)\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtools \u001b[38;5;241m=\u001b[39m tools\n\u001b[1;32m    255\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrollout_func \u001b[38;5;241m=\u001b[39m rollout_func\n\u001b[0;32m--> 257\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_init_vllm\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/trl/generation/vllm_generation.py:317\u001b[0m, in \u001b[0;36mVLLMGeneration._init_vllm\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    314\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvLLM does not support in-flight 8-bit quantization.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    316\u001b[0m \u001b[38;5;66;03m# Build LLM initialization kwargs\u001b[39;00m\n\u001b[0;32m--> 317\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm \u001b[38;5;241m=\u001b[39m \u001b[43mLLM\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    319\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensor_parallel_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor_parallel_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    320\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgpu_memory_utilization\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgpu_memory_utilization\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    321\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_model_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_model_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    322\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_num_seqs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_num_seqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    323\u001b[0m \u001b[43m    \u001b[49m\u001b[43menable_sleep_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menable_sleep_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    324\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_impl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_impl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    325\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdistributed_executor_backend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mexternal_launcher\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    326\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Feed identical seed for tp groups to ensure sampling results are the same across workers\u001b[39;49;00m\n\u001b[1;32m    327\u001b[0m \u001b[43m    \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess_index\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor_parallel_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    328\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Latest vLLM v1 memory profiler is misled by the high default value (i.e., 32768) - thinking there's not enough memory\u001b[39;49;00m\n\u001b[1;32m    329\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_num_batched_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4096\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    330\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Important so temperature scaling/logit tweaking affects the TIS log probs\u001b[39;49;00m\n\u001b[1;32m    331\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogprobs_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprocessed_logprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    332\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquantization\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquantization\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    333\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    334\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menable_sleep_mode:\n\u001b[1;32m    335\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm\u001b[38;5;241m.\u001b[39msleep(level\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/vllm/entrypoints/llm.py:297\u001b[0m, in \u001b[0;36mLLM.__init__\u001b[0;34m(self, model, runner, convert, tokenizer, tokenizer_mode, skip_tokenizer_init, trust_remote_code, allowed_local_media_path, allowed_media_domains, tensor_parallel_size, dtype, quantization, revision, tokenizer_revision, seed, gpu_memory_utilization, swap_space, cpu_offload_gb, enforce_eager, disable_custom_all_reduce, hf_token, hf_overrides, mm_processor_kwargs, pooler_config, override_pooler_config, structured_outputs_config, kv_cache_memory_bytes, compilation_config, logits_processors, **kwargs)\u001b[0m\n\u001b[1;32m    294\u001b[0m log_non_default_args(engine_args)\n\u001b[1;32m    296\u001b[0m \u001b[38;5;66;03m# Create the Engine (autoselects V0 vs V1)\u001b[39;00m\n\u001b[0;32m--> 297\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_engine \u001b[38;5;241m=\u001b[39m \u001b[43mLLMEngine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_engine_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    298\u001b[0m \u001b[43m    \u001b[49m\u001b[43mengine_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43musage_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mUsageContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLLM_CLASS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    299\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_engine)\n\u001b[1;32m    301\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest_counter \u001b[38;5;241m=\u001b[39m Counter()\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/vllm/v1/engine/llm_engine.py:177\u001b[0m, in \u001b[0;36mLLMEngine.from_engine_args\u001b[0;34m(cls, engine_args, usage_context, stat_loggers, enable_multiprocessing)\u001b[0m\n\u001b[1;32m    174\u001b[0m     enable_multiprocessing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;66;03m# Create the LLMEngine.\u001b[39;00m\n\u001b[0;32m--> 177\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvllm_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m           \u001b[49m\u001b[43mexecutor_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexecutor_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m           \u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mengine_args\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdisable_log_stats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m           \u001b[49m\u001b[43musage_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43musage_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[43m           \u001b[49m\u001b[43mstat_loggers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstat_loggers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[43m           \u001b[49m\u001b[43mmultiprocess_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menable_multiprocessing\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/vllm/v1/engine/llm_engine.py:114\u001b[0m, in \u001b[0;36mLLMEngine.__init__\u001b[0;34m(self, vllm_config, executor_class, log_stats, usage_context, stat_loggers, mm_registry, use_cached_outputs, multiprocess_mode)\u001b[0m\n\u001b[1;32m    111\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_processor\u001b[38;5;241m.\u001b[39mtracer \u001b[38;5;241m=\u001b[39m tracer\n\u001b[1;32m    113\u001b[0m \u001b[38;5;66;03m# EngineCore (gets EngineCoreRequests and gives EngineCoreOutputs)\u001b[39;00m\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine_core \u001b[38;5;241m=\u001b[39m \u001b[43mEngineCoreClient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake_client\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmultiprocess_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmultiprocess_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[43m    \u001b[49m\u001b[43masyncio_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvllm_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexecutor_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexecutor_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_stats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger_manager: Optional[StatLoggerManager] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_stats:\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/vllm/v1/engine/core_client.py:82\u001b[0m, in \u001b[0;36mEngineCoreClient.make_client\u001b[0;34m(multiprocess_mode, asyncio_mode, vllm_config, executor_class, log_stats)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m multiprocess_mode \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m asyncio_mode:\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SyncMPClient(vllm_config, executor_class, log_stats)\n\u001b[0;32m---> 82\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mInprocClient\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexecutor_class\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/vllm/v1/engine/core_client.py:245\u001b[0m, in \u001b[0;36mInprocClient.__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    244\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 245\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine_core \u001b[38;5;241m=\u001b[39m \u001b[43mEngineCore\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/vllm/v1/engine/core.py:92\u001b[0m, in \u001b[0;36mEngineCore.__init__\u001b[0;34m(self, vllm_config, executor_class, log_stats, executor_fail_callback)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mavailable_gpu_memory_for_kv_cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;66;03m# Setup KV Caches and update CacheConfig after profiling.\u001b[39;00m\n\u001b[1;32m     91\u001b[0m num_gpu_blocks, num_cpu_blocks, kv_cache_config \u001b[38;5;241m=\u001b[39m \\\n\u001b[0;32m---> 92\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_initialize_kv_caches\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     94\u001b[0m vllm_config\u001b[38;5;241m.\u001b[39mcache_config\u001b[38;5;241m.\u001b[39mnum_gpu_blocks \u001b[38;5;241m=\u001b[39m num_gpu_blocks\n\u001b[1;32m     95\u001b[0m vllm_config\u001b[38;5;241m.\u001b[39mcache_config\u001b[38;5;241m.\u001b[39mnum_cpu_blocks \u001b[38;5;241m=\u001b[39m num_cpu_blocks\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/vllm/v1/engine/core.py:199\u001b[0m, in \u001b[0;36mEngineCore._initialize_kv_caches\u001b[0;34m(self, vllm_config)\u001b[0m\n\u001b[1;32m    195\u001b[0m     available_gpu_memory \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(kv_cache_specs)\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(kv_cache_specs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(available_gpu_memory)\n\u001b[0;32m--> 199\u001b[0m kv_cache_configs \u001b[38;5;241m=\u001b[39m \u001b[43mget_kv_cache_configs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkv_cache_specs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[43m                                        \u001b[49m\u001b[43mavailable_gpu_memory\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    201\u001b[0m scheduler_kv_cache_config \u001b[38;5;241m=\u001b[39m generate_scheduler_kv_cache_config(\n\u001b[1;32m    202\u001b[0m     kv_cache_configs)\n\u001b[1;32m    203\u001b[0m num_gpu_blocks \u001b[38;5;241m=\u001b[39m scheduler_kv_cache_config\u001b[38;5;241m.\u001b[39mnum_blocks\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/vllm/v1/core/kv_cache_utils.py:1243\u001b[0m, in \u001b[0;36mget_kv_cache_configs\u001b[0;34m(vllm_config, kv_cache_specs, available_memory)\u001b[0m\n\u001b[1;32m   1240\u001b[0m \u001b[38;5;66;03m# Check if the available memory is enough for each worker.\u001b[39;00m\n\u001b[1;32m   1241\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m kv_cache_spec_one_worker, available_memory_one_worker \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\n\u001b[1;32m   1242\u001b[0m         kv_cache_specs, available_memory):\n\u001b[0;32m-> 1243\u001b[0m     \u001b[43mcheck_enough_kv_cache_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkv_cache_spec_one_worker\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1244\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mavailable_memory_one_worker\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1246\u001b[0m \u001b[38;5;66;03m# Merge the KV cache specs of all workers. Different PP stages may have\u001b[39;00m\n\u001b[1;32m   1247\u001b[0m \u001b[38;5;66;03m# different layer names, and different TP ranks of the same PP stage should\u001b[39;00m\n\u001b[1;32m   1248\u001b[0m \u001b[38;5;66;03m# have the same KV cache spec.\u001b[39;00m\n\u001b[1;32m   1249\u001b[0m merged_kv_cache_specs: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, KVCacheSpec] \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/vllm/v1/core/kv_cache_utils.py:716\u001b[0m, in \u001b[0;36mcheck_enough_kv_cache_memory\u001b[0;34m(vllm_config, kv_cache_spec, available_memory)\u001b[0m\n\u001b[1;32m    711\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m estimated_max_len \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    712\u001b[0m     estimated_msg \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    713\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBased on the available memory, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    714\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe estimated maximum model length is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimated_max_len\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 716\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    717\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTo serve at least one request with the models\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms max seq len \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    718\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmax_model_len\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m), (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mneeded_memory\u001b[38;5;241m/\u001b[39mGiB_bytes\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m GiB KV \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    719\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcache is needed, which is larger than the available KV cache \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    720\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mavailable_memory\u001b[38;5;241m/\u001b[39mGiB_bytes\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m GiB). \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    721\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimated_msg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    722\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTry increasing `gpu_memory_utilization` or decreasing \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    723\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`max_model_len` when initializing the engine.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: To serve at least one request with the models's max seq len (40960), (4.38 GiB KV cache is needed, which is larger than the available KV cache memory (0.96 GiB). Based on the available memory, the estimated maximum model length is 8976. Try increasing `gpu_memory_utilization` or decreasing `max_model_len` when initializing the engine."
     ]
    }
   ],
   "source": [
    "from trl import GRPOTrainer\n",
    "\n",
    "model_name=\"Qwen/Qwen3-1.7B\"\n",
    "\n",
    "trainer = GRPOTrainer(\n",
    "    model=model_name,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tools=[query_biogrid],\n",
    "    reward_funcs=[correctness_reward, structure_reward, query_reward],\n",
    "    args=grpo_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "r_qJ5UwLuzCG",
   "metadata": {},
   "source": [
    "Show memory stats before training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "DusT8JUaGmA6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "OTPkiz3fu0lp",
   "metadata": {},
   "source": [
    "And train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "NwI3buPOFMFk",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ITnLBLcTu2-p",
   "metadata": {},
   "source": [
    "Show memory stats after training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ftek6m4-GncK",
   "metadata": {},
   "outputs": [],
   "source": [
    "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
    "used_percentage = round(used_memory / max_memory * 100, 3)\n",
    "lora_percentage = round(used_memory_for_lora / max_memory * 100, 3)\n",
    "\n",
    "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
    "print(f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\")\n",
    "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
    "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
    "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
    "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "O6LAwznKu7mc",
   "metadata": {},
   "source": [
    "Let's save the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "idVgnNS1MWPr",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(output_dir)\n",
    "trainer.push_to_hub()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "707318cb",
   "metadata": {},
   "source": [
    "## Load the fine-tuned model and run inference using `smolagents`\n",
    "\n",
    "After fine-tuning the model with **GRPO (TRL)** for tool calling, we can test it at inference time using **`smolagents`**, a lightweight library for running multi-step agents.\n",
    "\n",
    "`smolagents` handles the agent loop for us:\n",
    "- Detecting tool calls generated by the model\n",
    "- Executing the corresponding tools (e.g. database queries)\n",
    "- Feeding the results back to the model until a final answer is produced\n",
    "\n",
    "> **Note**  \n",
    "> Using an agent framework is optional. The fine-tuned model can also be used directly with `transformers` by manually controlling the inference loop and executing the tools outside the model.\n",
    "> Agent frameworks are especially useful when the number of steps or tool calls is not fixed.\n",
    "\n",
    "We start by installing the required package:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab7fd5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/huggingface/smolagents.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24453572",
   "metadata": {},
   "source": [
    "We will use the `CodeAgent` class from `smolagents` to instantiate our agent.  \n",
    "First, we need to define the tool the agent can use. This is done using the `@tool` decorator.\n",
    "\n",
    "As shown below, the tool definition is **exactly the same** as the one used during GRPO training with TRL. This consistency is important: the model was trained to emit calls following this schema, and at inference time the agent simply executes the corresponding Python function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adcbbafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from smolagents import tool\n",
    "\n",
    "@tool\n",
    "def query_biogrid(sql_command: str) -> list[tuple]:\n",
    "    \"\"\"\n",
    "    Execute a read-only SQL query on the BioGRID database.\n",
    "\n",
    "    BioGRID is a curated biological database that compiles protein, genetic,\n",
    "    and chemical interactions from multiple organisms.\n",
    "\n",
    "    Args:\n",
    "        sql_command: A read-only SQL query to execute.\n",
    "\n",
    "    Returns:\n",
    "        A list of tuples containing the query results.\n",
    "    \"\"\"\n",
    "    with timeout(5):\n",
    "        conn = sqlite3.connect(\n",
    "            \"file:biogrid.db?mode=ro\",\n",
    "            uri=True,\n",
    "        )\n",
    "        cursor = conn.cursor()\n",
    "        try:\n",
    "            cursor.execute(sql_command)\n",
    "            results = cursor.fetchall()\n",
    "        finally:\n",
    "            conn.close()\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59721ad2",
   "metadata": {},
   "source": [
    "Now we can instantiate the agent using our fine-tuned model and the database tool defined above.\n",
    "We wrap the model with `TransformersModel` and pass both the model and the tool when creating the `CodeAgent`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ed8d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from smolagents import TransformersModel, CodeAgent\n",
    "\n",
    "model = TransformersModel(model_id=\"sergiopaniego/grpo_biogrid_qwen_3g-1.7b\", apply_chat_template_kwargs={\"enable_thinking\": False})\n",
    "\n",
    "# Create an agent with query_biogrid as tool\n",
    "agent = CodeAgent(tools=[query_biogrid], model=model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ba9462",
   "metadata": {},
   "source": [
    "Finally, we run the agent by passing the full prompt (including the instruction preamble and the question), exactly as it was used during training. This ensures the agent operates under the same context and assumptions learned with GRPO, allowing it to correctly decide when to query the database and how to format the final answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a3cdf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = agent.run(train_dataset[0]['prompt'][0]['content'])\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
